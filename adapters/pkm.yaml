# File: adapters/pkm.yaml
app: pkm
table: logs_pkm

source:
  kind: csv
  file_glob: "data/pkm/*.csv"     # pick up your local CSVs; CLI can override

parse:
  delimiter: "|"
  quotechar: '"'
  # exact order in each pipe-delimited row (when CSV has a single text column)
  fields:
    - user              # e.g., "(system)" or "Name Surname (login)"
    - id
    - subseq_id
    - message
    - audit_utc         # UTC timestamp string
    - action
    - type
    - label             # e.g., BS-CD20_RO7121932-000-006
    - version

actors:
  system_token: "(system)"
  login_regex: "\\((?P<login>[^)]+)\\)"      # captures login inside (...)
  display_regex: "^(?P<name>[^()|]+)"        # leading name before '('

target_schema:
  ts: TIMESTAMP
  actor: TEXT
  actor_display: TEXT
  product: TEXT
  action: TEXT
  type: TEXT
  id: TEXT
  subseq_id: TEXT
  version: TEXT
  message: TEXT

mappings:
  ts: audit_utc
  actor: "login_or_system"        # derived: login if present, else system token or display name
  actor_display: "display_name"   # derived from user
  product: label
  message: message
  action: action
  type: type
  id: id
  subseq_id: subseq_id
  version: version

constraints:
  require_fields: ["message"]      # fail fast if message is missing
  drop_empty_ts: false             # keep rows even if ts missing (filter later in SQL)

indexes:
  - ["ts"]
  - ["actor","product","ts"]