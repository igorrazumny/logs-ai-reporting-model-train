# File: docker-compose.yml

services:
  # Local LLM (single 8B model) â€” used by planner/presenter.
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama

  app:
    build:
      context: .
      args:
        BASE: "cpu"          # set to "cuda" if you add a CUDA base later
    image: logs-train:cpu
    container_name: logs-train
    environment:
      - PYTHONUNBUFFERED=1
      - LLM_HOST=http://ollama:11434
      - LLM_MODEL=llama3.1-8b-instruct-q4_K_M
    volumes:
      - ./src:/app/src            # live-mount code for fast dev, no rebuilds
      - ./data:/app/data          # local CSVs (gitignored)
      - ./outputs:/app/outputs    # DuckDB, logs (gitignored)
      - ./adapters:/app/adapters  # YAML adapters
      - ./docs:/app/docs          # optional, if UI links docs
    depends_on:
      - ollama
    # We invoke modules explicitly via `make` (overrides entrypoint/cmd):
    #   make load CSV="data/pkm/2020-04 Source Logs.csv"
    #   make ui

volumes:
  ollama: